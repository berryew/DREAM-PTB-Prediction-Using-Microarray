{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DREAM_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berryew/DREAM-PTD-Prediction-Using-Microarray/blob/master/DREAM_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EWp2Nar7U7C",
        "colab_type": "text"
      },
      "source": [
        "### Install packages, import them, and load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z__0dYB7HgP",
        "colab_type": "code",
        "outputId": "d4b0ff8d-1a9c-46be-95a7-d61fe4cba476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "!pip install Biopython\n",
        "!pip install lifelines\n",
        "!pip install imblearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Biopython in /usr/local/lib/python3.6/dist-packages (1.75)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Biopython) (1.17.4)\n",
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.6/dist-packages (0.23.4)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (0.25.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (3.1.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.3.3)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.6/dist-packages (from lifelines) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.17.4)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->lifelines) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (2.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.23.0->lifelines) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0->lifelines) (42.0.2)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dEJi12BtjzJ",
        "colab_type": "code",
        "outputId": "b7404201-606c-41e8-a650-cb6e3ae9110e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5OTB04ftemp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "DATA = Path(\"/content/drive/My Drive/E4060/DREAM/data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz3Fyvn0uFoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "61126d2b-d6c5-485e-de63-41ee9a3aa9a9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scipy import interp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from Bio.Cluster import kcluster\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLgxAE1LuBIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the microarray\n",
        "# Just `eset_SC2_v20.csv` and `anoSC2_v20_nokey.csv` will be sufficient. \n",
        "# `eset_SC2_v20.csv` contains information of both `HuGene21ST_RMA.csv` and `HTA20_RMA.csv`\n",
        "eset = pd.read_csv(DATA / \"eset_SC2_v20.csv\", index_col=0)\n",
        "# load the annotation\n",
        "annot = pd.read_csv(DATA / \"anoSC2_v20_nokey.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c525Cxlh7oqY",
        "colab_type": "text"
      },
      "source": [
        "### Define SampleID-IndividualID map, parameters and models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgUMbEDq7XtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a SampleID-IndividualID map, so that we can group by IndividualID and calculate the average\n",
        "sample_to_individual = pd.Series(annot.IndividualID.values, index=annot.SampleID).to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hS-rgxa75jv",
        "colab_type": "text"
      },
      "source": [
        "#### Survival model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYoWTKyE7u1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define xgboost (survival model) parameters\n",
        "xgb_params = [\n",
        "{\n",
        "    \"eta\": 0.01,\n",
        "    \"subsample\": 0.5,\n",
        "    \"colsample_bytree\": 0.75,\n",
        "    \"max_depth\": 5,\n",
        "    \"objective\": \"survival:cox\",\n",
        "    \"eval_metric\": \"cox-nloglik\",\n",
        "    \"seed\": 4060\n",
        "},\n",
        "{\n",
        "    \"eta\": 0.01,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"max_depth\": 10,\n",
        "    \"objective\": \"survival:cox\",\n",
        "    \"eval_metric\": \"cox-nloglik\",\n",
        "    \"seed\": 4060\n",
        "}, \n",
        "{\n",
        "    \"eta\": 0.01,\n",
        "    \"subsample\": 0.5,\n",
        "    \"colsample_bytree\": 0.5,\n",
        "    \"max_depth\": 5,\n",
        "    \"objective\": \"survival:cox\",\n",
        "    \"eval_metric\": \"cox-nloglik\",\n",
        "    \"seed\": 4060\n",
        "}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq71P2br8IAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "`X_train`: training set input\n",
        "`y_train`: training set output\n",
        "`X_test`: validation input\n",
        "`xgb_params`: tune parameters inside the cv of model training\n",
        "output the normalized scores, as probabilities\n",
        "'''\n",
        "def xgb_prediction(X_train, y_train, X_test, xgb_params):\n",
        "    cv_scores = []\n",
        "    cv_rounds = []\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    for params in xgb_params:\n",
        "        cvout = xgb.cv(params, dtrain, num_boost_round=500, nfold=5,\n",
        "                    early_stopping_rounds=10)\n",
        "        # get the best (minimum) loss score\n",
        "        cv_scores.append(cvout[\"test-cox-nloglik-mean\"].min())\n",
        "        cv_rounds.append(cvout[\"test-cox-nloglik-mean\"].idxmin())\n",
        "    print(cv_scores, cv_rounds)\n",
        "    xgbmodel = xgb.train(xgb_params[np.argmin(cv_scores)],\n",
        "                     dtrain, num_boost_round=cv_rounds[np.argmin(cv_scores)])\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "    pred = xgbmodel.predict(dtest)\n",
        "    # normalize the risk score, so that it can be treated as a probability\n",
        "    return normalize(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Zh09Sz8KRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "`X` defines the whole dataset, train and test are the index of validation split\n",
        "`method` defines clustering methods, either kmeans or hierarchical \n",
        "    * the parameters of hierarchical clustering can be set as hierarchical.euclidean.average for example\n",
        "`n_cluster` defines the maximum clusters\n",
        "    * based on the cllustering, leave out clusters with much less genes, and generate the features by calculate the means of each cluster\n",
        "`df` decides the type of output, pandas DataFrame or numpy array\n",
        "'''\n",
        "def mean_by_cluster_sa(X, train, test, n_cluster=5, method='kmeans', df=True):\n",
        "    \n",
        "    # clustering, defined by argument `method`\n",
        "    if method == 'kmeans':\n",
        "        clusterid, error, nfound = kcluster(np.array(X.iloc[train].transpose()), nclusters=n_cluster)\n",
        "    elif method.startswith('hierarchical'):\n",
        "        _, affinity, linkage = method.split(\".\")\n",
        "        cluster = AgglomerativeClustering(n_clusters=n_cluster, affinity=affinity, linkage=linkage)\n",
        "        cluster.fit_predict(np.array(X.iloc[train].transpose()))\n",
        "        clusterid = cluster.labels_\n",
        "\n",
        "    metagenes = X.transpose().copy()\n",
        "    metagenes = metagenes.reset_index(drop=True)\n",
        "    metagenes['id'] = clusterid\n",
        "    metagenes['size'] = metagenes.groupby('id').size()\n",
        "    # candidate_condition = metagenes['size'] > 0.4 * X.shape[1]//n_cluster\n",
        "    candidate_condition = metagenes['size'] > max(0.4 * X.shape[1]//n_cluster, 3)\n",
        "    metagenes = metagenes.groupby('id').mean()\n",
        "    # print(metagenes['size'] > 0.5 * X.shape[0]//n_cluster)\n",
        "    metagenes = metagenes[candidate_condition]\n",
        "    print('Before | After deleting clusters with much less objects: {} | {}'.format(Counter(clusterid), metagenes.shape[0]))\n",
        "    if df:\n",
        "        return metagenes.iloc[:, train].transpose(), metagenes.iloc[:, test].transpose()\n",
        "    else:\n",
        "        return np.array(metagenes.iloc[:, train].transpose()), np.array(metagenes.iloc[:, test].transpose())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "229ClvnX8NzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "normalize the value to range from 0 to 1\n",
        "'''\n",
        "def normalize(array):\n",
        "    return (array - min(array)) / (max(array) - min(array))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nkYc5AW8Obf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cidx_df = pd.read_csv(DATA / \"Concordance_results_T2.csv\", index_col=0)\n",
        "cidx_top_genes = cidx_df.index[:100]\n",
        "\n",
        "T2 = annot.sort_values('GA', ascending=False).drop_duplicates(['IndividualID'])\n",
        "train_id_T2 = T2[T2.Train==1].SampleID\n",
        "test_id_T2 = T2[T2.Train==0].SampleID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St2-V1Bk84LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_sa = eset.loc[cidx_top_genes, eset.columns.isin(train_id_T2)].transpose()\n",
        "y_sa = annot.loc[annot.SampleID.isin(train_id_T2), ['Group', 'TTD']]\n",
        "y_sa['Group'] = y_sa.Group != 'Control'\n",
        "y_sa['Event'] = True\n",
        "y_sa.index = X_sa.index\n",
        "X_y_sa = pd.concat([X_sa, y_sa], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLVy9I4U894B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 4060\n",
        "X_train, y_TTD, y_Group = shuffle(X_sa, y_sa.TTD.values, y_sa.Group.values, random_state=seed)\n",
        "X_test = eset.loc[cidx_top_genes, eset.columns.isin(test_id_T2)].transpose()\n",
        "X = pd.concat([X_train, X_test], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTr6gQwQ9WXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0d79b222-946a-4c5c-8ae7-ecf2cff70b62"
      },
      "source": [
        "train = range(len(X_train))\n",
        "test = range(len(X_train), len(X))\n",
        "\n",
        "X_train, X_test = mean_by_cluster_sa(X, train, test, 10, 'hierarchical.euclidean.complete')\n",
        "print(\"True samples in training: {}\".format(sum(y_Group[train])))\n",
        "probas_ = xgb_prediction(X_train, y_TTD, X_test, xgb_params)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before | After deleting clusters with much less objects: Counter({0: 31, 2: 18, 5: 13, 3: 12, 1: 9, 4: 8, 8: 6, 6: 1, 7: 1, 9: 1}) | 7\n",
            "True samples in training: 64\n",
            "[2.7647386000000003, 2.7554108, 2.7600056] [64, 74, 109]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cWjSWwAfhI",
        "colab_type": "text"
      },
      "source": [
        "##### Create a dataframe `prediction_sa`, with columns `IndividualID, sPTD, PPROM`\n",
        "For survival analysis, the goal is to prediction the duration of event (delivery). Therefore, two predictions (sPTD and PPROM) are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuCzHqEz_1_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "53c08223-2215-4d67-b8a5-10c69b322202"
      },
      "source": [
        "prediction_sa = pd.DataFrame({'SampleID': X_test.index, 'sPTD': probas_, 'PPROM': probas_})\n",
        "prediction_sa['IndividualID'] = prediction_sa['SampleID'].map(sample_to_individual)\n",
        "prediction_sa = prediction_sa.drop(columns='SampleID').set_index('IndividualID')\n",
        "prediction_sa.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sPTD</th>\n",
              "      <th>PPROM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IndividualID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18275</th>\n",
              "      <td>0.600750</td>\n",
              "      <td>0.600750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18427</th>\n",
              "      <td>0.238119</td>\n",
              "      <td>0.238119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18473</th>\n",
              "      <td>0.473468</td>\n",
              "      <td>0.473468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19086</th>\n",
              "      <td>0.308764</td>\n",
              "      <td>0.308764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19217</th>\n",
              "      <td>0.005074</td>\n",
              "      <td>0.005074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  sPTD     PPROM\n",
              "IndividualID                    \n",
              "18275         0.600750  0.600750\n",
              "18427         0.238119  0.238119\n",
              "18473         0.473468  0.473468\n",
              "19086         0.308764  0.308764\n",
              "19217         0.005074  0.005074"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFfLX-G8AzTw",
        "colab_type": "text"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzBVZiMcBO2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "downsampling the majority class\n",
        "`X_y` defines the training matrix, row: samples, column: genes + Group\n",
        "`sep` defines the index to split columns into genes + annotation, e.g. Group\n",
        "`df` defines the type of output\n",
        "`seed` defines the random state\n",
        "`shuffle` defines if shuffuling is needed\n",
        "'''\n",
        "def downsampling(X_y, sep=-1, df=False, seed=47, shuffle=False):\n",
        "    True_samples = X_y[X_y.Group].index\n",
        "\n",
        "    # Shuffle the Dataset.\n",
        "    X_y = X_y.sample(frac=1, random_state=7)\n",
        "\n",
        "    # Put all the positive samples in a separate dataset.\n",
        "    True_X_y = X_y.loc[X_y.index.isin(True_samples)]\n",
        "\n",
        "    # Randomly select observations from the negative samples\n",
        "    False_X_y = X_y.loc[~X_y.index.isin(True_samples)].sample(n=len(True_samples), random_state=seed)\n",
        "\n",
        "    # Concatenate both dataframes\n",
        "    downsampled_X_y = pd.concat([True_X_y, False_X_y])\n",
        "\n",
        "    # based on the type of output: dataframe or np.array\n",
        "    if df:\n",
        "        X, y = downsampled_X_y.iloc[:, :sep], downsampled_X_y.iloc[:, sep:]\n",
        "    else:\n",
        "        X, y = np.array(downsampled_X_y.iloc[:, :sep]), np.array(downsampled_X_y.iloc[:, sep:])\n",
        "\n",
        "    \n",
        "    # based on whether the output need shuffling\n",
        "    if shuffle:\n",
        "        return shuffle(X, y, random_state=seed)\n",
        "    else:\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj0SETJHBTvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "oversampling the minority class, use SMOTE from the library `imblearn`\n",
        "`X_y` defines the training matrix, row: samples, column: genes + Group\n",
        "`sep` defines the index to split columns into genes + annotation, e.g. Group\n",
        "`df` defines the type of output\n",
        "`seed` defines the random state\n",
        "`shuffle` defines if shuffuling is needed\n",
        "'''\n",
        "def oversampling(X_y, sep=-1, df=False, seed=47, shuffle=False):\n",
        "    # Resample the minority class.\n",
        "    sm = SMOTE(sampling_strategy='minority', random_state=4060)\n",
        "\n",
        "    # Fit the model to generate the data.\n",
        "    X, y = sm.fit_sample(X_y.iloc[:, :sep], X_y.iloc[:, sep:])\n",
        "    \n",
        "    # based on the type of output: dataframe or np.array\n",
        "    if df:\n",
        "        X = pd.DataFrame(X, columns=X_y.columns[:-1])\n",
        "        y = pd.DataFrame(y, columns=['Group'])\n",
        "    \n",
        "    # based on whether the output need shuffling\n",
        "    if shuffle:\n",
        "        return shuffle(X, y, random_state=seed)\n",
        "    else:\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib4t2qslBWL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "`X_train` defines the traning set\n",
        "`X_test` defines the test set\n",
        "`method` defines clustering methods, either kmeans or hierarchical \n",
        "    * the parameters of hierarchical clustering can be set as hierarchical.euclidean.average for example\n",
        "`n_cluster` defines the maximum clusters\n",
        "    * based on the cllustering, leave out clusters with much less genes, and generate the features by calculate the means of each cluster\n",
        "`df` decides the type of output, pandas DataFrame or numpy array\n",
        "'''\n",
        "def mean_by_cluster_clf(X_train, X_test, n_cluster=5, method='kmeans', df=True):\n",
        "    \n",
        "    # clustering, defined by argument `method`\n",
        "    if method == 'kmeans':\n",
        "        clusterid, error, nfound = kcluster(np.array(X_train.transpose()), nclusters=n_cluster)\n",
        "    elif method.startswith('hierarchical'):\n",
        "        _, affinity, linkage = method.split(\".\")\n",
        "        cluster = AgglomerativeClustering(n_clusters=n_cluster, affinity=affinity, linkage=linkage)\n",
        "        cluster.fit_predict(np.array(X_train.transpose()))\n",
        "        clusterid = cluster.labels_\n",
        "\n",
        "    sep = len(X_train)\n",
        "\n",
        "    metagenes = pd.concat([X_train, X_test], axis=0, sort=False).transpose().copy()\n",
        "    metagenes = metagenes.reset_index(drop=True)\n",
        "    metagenes['id'] = clusterid\n",
        "    metagenes['size'] = metagenes.groupby('id').size()\n",
        "    # candidate_condition = metagenes['size'] > 0.4 * X.shape[1]//n_cluster\n",
        "    candidate_condition = metagenes['size'] > max(0.4 * X.shape[1]//n_cluster, 3)\n",
        "    # candidate_condition = metagenes['size'] > 4\n",
        "    # print(metagenes.head(1))\n",
        "    metagenes = metagenes.groupby('id').mean()\n",
        "    # print(metagenes['size'] > 0.5 * X.shape[0]//n_cluster)\n",
        "    metagenes = metagenes[candidate_condition].drop(columns='size')\n",
        "    print('Before | After deleting clusters with much less objects: {} | {}'.format(Counter(clusterid), metagenes.shape[0]))\n",
        "    \n",
        "    if df:\n",
        "        return metagenes.iloc[:, :sep].transpose(), metagenes.iloc[:, sep:].transpose()\n",
        "    else:\n",
        "        return np.array(metagenes.iloc[:, :sep].transpose()), np.array(metagenes.iloc[:, sep:].transpose())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEoeiY_WBkbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_id = annot[annot.Train == 1]['SampleID']\n",
        "test_id = annot[annot.Train == 0]['SampleID']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvvsTFSMBhbH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "5c15dc39-019e-49cd-b155-54bb1b0b944d"
      },
      "source": [
        "categories = ['sPTD', 'PPROM']\n",
        "\n",
        "prediction_clf = pd.DataFrame({'SampleID': test_id})\n",
        "prediction_clf['IndividualID'] = prediction_clf['SampleID'].map(sample_to_individual)\n",
        "\n",
        "for category in categories:\n",
        "    file_name = \"topTable1000_\" + category + \".csv\"\n",
        "    topTable = pd.read_csv(DATA / file_name, index_col=0)\n",
        "    top_genes = topTable.index[:100]\n",
        "\n",
        "    X_clf = eset.loc[top_genes, eset.columns.isin(train_id)].transpose()\n",
        "    y_clf = annot.loc[annot.SampleID.isin(train_id), ['Group']]\n",
        "    y_clf['Group'] = y_clf['Group'] == category\n",
        "    X_y_clf = pd.concat([X_clf.reset_index(), y_clf.reset_index(drop=True)], axis=1)\n",
        "    X_y_clf = X_y_clf.set_index('index')\n",
        "\n",
        "    seed = 4060\n",
        "    random_state = np.random.RandomState(seed//2)\n",
        "    X_train, y_train = oversampling(X_y_clf, df=True)\n",
        "    X_train, y_train = shuffle(X_train, y_train, random_state=seed)\n",
        "    y_train = np.ravel(y_train)\n",
        "    X_test = eset.loc[top_genes, eset.columns.isin(test_id)].transpose()\n",
        "    \n",
        "    X = pd.concat([X_train, X_test], axis=0)\n",
        "    train = range(len(X_train))\n",
        "    test = range(len(X_train), len(X))\n",
        "\n",
        "    classifier = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
        "    \n",
        "    X_train, X_test = mean_by_cluster_sa(X, train, test, 10, 'hierarchical.euclidean.complete', df=False)\n",
        "    print(\"True samples in training: {}\".format(sum(y_train)))\n",
        "    probas_ = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
        "    prediction_clf[category] = probas_[:, 1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before | After deleting clusters with much less objects: Counter({9: 18, 3: 18, 6: 18, 2: 13, 1: 10, 7: 9, 0: 8, 5: 3, 4: 2, 8: 1}) | 7\n",
            "True samples in training: 380\n",
            "Before | After deleting clusters with much less objects: Counter({2: 23, 6: 20, 1: 19, 3: 16, 4: 7, 0: 7, 5: 4, 7: 2, 8: 1, 9: 1}) | 6\n",
            "True samples in training: 340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujRtoWpzKdfm",
        "colab_type": "text"
      },
      "source": [
        "##### Create a dataframe prediction_clf, with columns `IndividualID, sPTD, PPROM`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G54t1i35GPJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "376406b1-206e-4b57-e86e-cadfa0820454"
      },
      "source": [
        "prediction_clf = prediction_clf.groupby('IndividualID').mean()\n",
        "prediction_clf.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sPTD</th>\n",
              "      <th>PPROM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IndividualID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18223</th>\n",
              "      <td>0.726774</td>\n",
              "      <td>0.620952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18275</th>\n",
              "      <td>0.084437</td>\n",
              "      <td>0.414845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18427</th>\n",
              "      <td>0.053158</td>\n",
              "      <td>0.535086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18453</th>\n",
              "      <td>0.192672</td>\n",
              "      <td>0.398344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18473</th>\n",
              "      <td>0.115525</td>\n",
              "      <td>0.413953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  sPTD     PPROM\n",
              "IndividualID                    \n",
              "18223         0.726774  0.620952\n",
              "18275         0.084437  0.414845\n",
              "18427         0.053158  0.535086\n",
              "18453         0.192672  0.398344\n",
              "18473         0.115525  0.413953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoGIR-DAKrz3",
        "colab_type": "text"
      },
      "source": [
        "#### Combine two models, and obtain the averages respectively "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF3t5uezG8hG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d2071d1f-8544-4a6b-83ae-ab9d8c7d519e"
      },
      "source": [
        "prediction_combine = pd.concat([prediction_sa, prediction_clf])\n",
        "prediction_combine = prediction_combine.groupby(prediction_combine.index).mean()\n",
        "prediction_combine.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sPTD</th>\n",
              "      <th>PPROM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IndividualID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18223</th>\n",
              "      <td>0.455851</td>\n",
              "      <td>0.402940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18275</th>\n",
              "      <td>0.342594</td>\n",
              "      <td>0.507797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18427</th>\n",
              "      <td>0.145639</td>\n",
              "      <td>0.386603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18453</th>\n",
              "      <td>0.211595</td>\n",
              "      <td>0.314430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18473</th>\n",
              "      <td>0.294497</td>\n",
              "      <td>0.443711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  sPTD     PPROM\n",
              "IndividualID                    \n",
              "18223         0.455851  0.402940\n",
              "18275         0.342594  0.507797\n",
              "18427         0.145639  0.386603\n",
              "18453         0.211595  0.314430\n",
              "18473         0.294497  0.443711"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}